Using Clad
***********

This section briefly describes all the key functionalities offered by Clad.
If you are just getting started with Clad, then this is the best place to start.
You may want to skim some sections on the first read. 

In case you haven't installed Clad already, then please do before proceeding 
with this guide. Visit :doc:`Clad installation and usage <InstallationAndUsage>` 
to know more about installing clad.

Let's get started.

Automatic Differentiation
===========================

Clad differentiation functions takes a function as an input and returns a 
``clad::CladFunction`` object that contains information about the generated 
derived function. Generated derived function can be called by calling the 
`.execute` method on the corresponding `clad::CladFunction` object.

Clad consists of 4 primary automatic differentiation functions:

- ``clad::differentiate`` -- Primary forward mode automatic differentiation
- ``clad::gradient`` -- Primary reverse mode automatic differentiation
- ``clad::hessian``
- ``clad::jacobian``

Each of these functions will be explored in this guide.

.. todo::

   Perhaps add example use before proceeding with different differentiation modes.


Forward Mode Automatic Differentiation
----------------------------------------

Forward mode AD computes derivatives of all the output parameters of a program with 
respect to an input parameter. The input parameter with respect to which differentiation
takes place is termed as independent parameter. 
Mathematically, the forward mode AD allows the efficient computation of columns of 
the jacobian matrix.

``clad::differentiate`` provides the forward mode differentiation functionality. 
It takes as input, a source function and independent parameter information,
parameter with respect to which differentiation should take place, and returns a ``clad::CladFunction`` object. 
A call to ``clad::differentiate`` tells Clad to generate a function that computes the 
derivatives of the source function with respect to the independent parameter. We will
call the function generated by Clad that computes derivatives as derived function. 
``clad::CladFunction`` is a simple wrapper over the derived function and provides convenient access to it.
Generated derived function is executed by calling the ``.execute`` member 
function on the associated ``clad::CladFunction`` object.

An example that demonstrates the usage of 
``clad::differentiate``::

   #include "clad/Differentiator/Differentiator.h"
   #include <iostream>

   double fn(double x, double y) {
     return x*x + y*y;
   }

   int main() {
     // differentiate 'fn' w.r.t 'x'.
     auto d_fn_1 = clad::differentiate(fn, "x");
  
     // computes derivative of 'fn' w.r.t 'x' when (x, y) = (3, 4).
     std::cout<<d_fn_1.execute(3, 4)<<"\n"; // prints 6
   }

The independent parameter can be specified either using the parameter name or
the parameter index. Parameter indexing starts from 0. Therefore for 
the example shown above, the two differentiation calls shown below are
equivalent::

  clad::differentiate(fn, "x");

and:: 
  
  clad::differentiate(fn, 0);

The derived function is executed by calling the ``.execute`` method on the associated ``clad::CladFunction`` object. 
Clad forward mode differentiated functions returns the computed derivatives,
this behaviour is unique among other types of derived functions generated by Clad.

Clad can also differentiate with respect to an array element. The following
example demonstrates this::

  double fn_arr(double* arr, int n) {
    double res = 0;
    for (int i=0; i<n-1; ++i)
      res += arr[i] * arr[i+1];
    return res;
  }

  int main() {
    // Differentiate 'fn_arr' w.r.t element '1' of the 'arr' parameter
    auto d_fn_arr = clad::differentiate(fn_arr, "arr[1]");
    double arr[5] = {1, 2, 3, 4, 5};
    std::cout<<d_fn_arr.execute(arr, 5)<<"\n";  // prints 4
  }

Arbitrary higher-order derivatives can also be computed using forward mode 
differentiation. The following examples demonstrate computation of higher-order derivatives::

  double fn(double i) { return i * i * i * i; }

  int main() {
    // Differentiate 3rd order higher derivative of 'fn_arr' w.r.t parameter 'i'
    auto d_fn_3 = clad::differentiate<3>(fn, "i");
    std::cout << d_fn_3.execute(3) << "\n"; // prints 72.00
  }

.. note::

   Forward mode AD can only be used to differentiate with respect to a single 
   value. For differentiating with respect to multiple values (parameters), 
   reverse-mode AD must be used..
  
Visit API reference of :ref:`clad::differentiate<api_reference_clad_differentiate>`
for more details.

Reverse Mode Automatic Differentiation
----------------------------------------

Hessian Computation
----------------------

Clad can directly compute the 
`hessian matrix <https://en.wikipedia.org/wiki/Hessian_matrix>`_ of a
function using the ``clad::hessian`` function.

.. figure:: ../_static/hessian-matrix.png
  :width: 400
  :align: center
  :alt: Hessian matrix image taken from wikipedia
  
  Hessian matrix when specified parameters are 
  (x\ :sub:`1`\ , x\ :sub:`2`\ , ..., x\ :sub:`n`\ ).

``clad::hessian`` provides the hessian computation functionality. 
The ``clad::hessian`` function takes a source function as input, and optionally, 
information about independent parameters -- with respect to which hessian should be computed.

Internally, ``clad::hessian`` uses both the forward mode AD and the 
reverse mode AD to efficiently compute hessian matrix.  

An example that demonstrates the usage of ``clad::hessian``::

  #include "clad/Differentiator/Differentiator.h"

  double kinetic_energy(double mass, double velocity) {
    return 0.5 * mass * velocity * velocity;
  }

  int main() {
    // Tells clad to generate a function that computes hessian matrix of the function 
    // 'kinetic_energy' with respect to all the input paramters.
    auto hessian_one = clad::hessian(kinetic_energy);

    // Can manually specify independent arguments
    auto hessian_two = clad::hessian(kinetic_energy, "mass, velocity");

    // Creates an empty matrix to store the Hessian in
    // Must have enough space, 2 independent variables requires 4 elements (2^2=4)
    double matrix[4];

    clad::array_ref<double> matrix_ref(matrix, 4);

    // Prints the generated Hessian function
    hessian_one.dump();
    hessian_two.dump();

    // Substitutes these values into the Hessian function and pipes the result
    // into the matrix variable.
    hessian_one.execute(10, 2, matrix_ref);
    hessian_two.execute(5, 1, matrix_ref);
  }

Few important things to note about ``clad::hessian``:

- If no independent variable information is provided, then hessian matrix is 
  computed by taking all differentiable function parameters as independent
  variables.

- Independent argument information is provided as a string literal with comma
  separated names of function parameters. For example::

    double product(double i, double j, double k) {
      return i*j*k;
    }
    // Tells Clad to generate a function that computes hessian matrix of the
    // 'product' function with respect to the input parameters 'i' and 'j'.
    auto d_fn = clad::hessian(product, "i, j");

- ``clad::hessian`` also supports differentiating w.r.t multiple paramters.

::
  
  auto d_fn = clad::hessian(fn);
  double hessian_matrix[4] = {};
  clad::array_ref<double> hessian_matrix_ref(hessian_matrix, 4);
  d_fn.execute(3, 5, hessian_matrix);

- hessian matrix array should be passed as the last argument to the call 
  to the ``CladFunction::execute`` as shown in the above code sample. It will store the hessian matrix computed 
  by the derived function. The hessian matrix array size should at least be as big as the size 
  required to store the hessian matrix. Passing an array less than the required size will result in undefined behaviour.

.. todo::

   Add details for computing hessian of array ranges.

Jacobian Computation
----------------------

Array Support 
----------------

Differentiating Functors and Lambdas
-------------------------------------

Despite significant differences, differentiating functors and lambda
expressions is remarkably similar to differentiating ordinary functions.
Similarly, computing the hessian matrix and jacobian matrix of functors and
lambda expressions is also similar to computing hessian matrix and 
jacobian matrix of ordinary functions.

Differentiating functors and lambdas means differentiating the call operator 
(operator()) member function defined by the functor and lambda type and 
executing the differentiated function using a reference to the functor object.

An example that demonstrates the differentiation of functors::

  #include "clad/Differentiator/Differentiator.h"
  
  // A class type with user-defined call operator
  class Equation {
    double m_x, m_y;
  
    public:
    Equation(double x, double y) : m_x(x), m_y(y) {}
    double operator()(double i, double j) {
      return m_x*i*j + m_y*i*j;
    }
    void setX(double x) {
      m_x = x;
    }
  };
  
  int main() {
    Equation E(3, 5);
  
    // Functor is an object of any type which have user defined call operator.
    //
    // Clad differentiation functions can directly differentiate functors.
    // Functors can be passed to clad differentiation functions in two distinct
    // ways:
  
    // 1) Pass by reference
    // differentiates 'E' wrt parameter 'i'
    // object 'E' is saved in the 'CladFunction' object 'd_E'
    auto d_E = clad::differentiate(E, "i");
  
    // 2) Pass as pointers
    // differentiates 'E' wrt parameter 'i'
    // object 'E' is saved in the 'CladFunction' object 'd_E_pointer'
    auto d_E_pointer = clad::differentiate(&E, "i");
  
    // calculate differentiation of 'E' when (i, j) = (7, 9)
    double res1 = d_E.execute(7, 9);  // prints 66
    double res2 = d_E_pointer.execute(7, 9);  // prints 66
  }

Functors and lambda expressions can be passed both by reference and by pointers.
Therefore, the two differentiation calls shown below are equivalent::

  Experiment E;  // a functor
  // passing function by reference
  auto d_E = clad::differentiate(E, "i");

and::

  Experiment E;  // a functor
  // passing function by pointer
  auto d_E = clad::differentiate(&E, "i");

An example that demonstrates differentiation of lambda expressions::

  int main() {
    auto lambda = [](double i, double j) {
      return i*j;
    };
    // Pass by reference
    auto lambda_grad = clad::gradient(lambda);
    // Can be passed by pointer as well!!
    auto lambda_grad_pointer = clad::gradient(&lambda);

    double d_i_1, d_j_1, d_i_2, d_j_2;
    d_i_1 = d_j_1 = d_i_2 = d_j_2 = 0;

    lambda_grad.execute(3, 5, &d_i_1, &d_j_1);
    lambda_grad_pointer.execute(3, 5, &d_i_2, &d_j_2);

    std::cout<<d_i_1<<" "<<d_j_1<<"\n"; // prints 5 3
    std::cout<<d_i_2<<" "<<d_j_2<<"\n"; // prints 5 3
  }

.. note::

   Functor class should not contain multiple overloaded call operators. 
   This restriction will be removed in the future.  

Differentiable Class Types
----------------------------

.. note:: 

   This feature is currently experimental. Expect some adventures while
   using it. If you do decide to go on this adventure, please consider giving 
   your review as well on how we can improve this functionality.

One of the main goals of Clad is to be able to differentiate existing codebases
with minimal boilerplate code. Most existing codebases invariably use several
data structures for representing different information at various stages of computations.
Thus, we are adding support for differentiating class type objects for convenient and effective
differentiation of existing codebases. 

Before going any further, let's first understand how data structures and functions 
relate to their mathematical counterparts, and what does it mean for a data structure or a function
to be differentiable.
As per the principles of calculus and from purely mathematical perspective, only mathematical functions, are 
differentiable. Intuitively, data structures represent a mathematical space, and are thus, not 
differentiable from mathematical point of view. On the other hand, functions represent mathematical functions and can thus,
be differentiable. Please note that not all functions represent a mathematical space. We will call a function, a differentiable function, if it represents a mathematical function 
and can be differentiated.
We will call a type, a differentiable type, if Clad can perform calculus on the 
values of this type. A differentiable function should only use differentiable types
in its definition. 

.. math::

   F : X \longrightarrow Y

where

.. math::

   X = (x_0, x_1, x_2, ...) \\
   Y = (y_0, y_1, y_2, ...)


For a class type to be differentiable, it should satisfy the following rules:

- Represent a real vector space.
- Have a default constructor that zero initialises the object of
  the class.
  Class objects initialised by the default constructor should represent
  0 tangent vector -- that is, all real data members should be equal to 0.
- Copy initialisation should perform deep copy initialisation. For example, 
  after performing the initialisation ``a(b)``:, 1) there should be no shared 
  resource between ``a`` and ``b``, and 2) values of all the associated data
  members of ``a`` and ``b`` should be equal.
- Assignment operator should performs deep copy. 
  For example, after performing the assignment ``a = b;``: there should be no 
  shared resource between ``a`` and ``b``; and values of all the associated data
  members of ``a`` and ``b`` should be equal.
  

In general, type of derivative of a variable of type 'YType' with respect to
a variable of type 'XType' is a function of both 'YType' and 'XType'. Therefore,
:math:`DerivativeType = f(YType, XType)`. Intuitively, derivative type should be
able to represent all the derivatives that are obtained on differentiating a variable
``y`` with respect to a variable ``x``. We will obtain more than one derivative if either
or both of ``x`` and ``y`` are aggregate types.

In case when both ``y`` and ``x`` are built-in scalar numerical type, as your 
intuition probably suggests, the derivative type is also a built-in scalar 
numerical type. Things get more complicated when one or both the types
are aggregate types. When used in a computation, aggregate types -- or more generally, 
class types -- represent a mathematical space. In the specific case when either of ``y`` 
or ``x`` is a built-in numerical scalar type, the derivative type can be considered
to be the other type. For example, consider the following aggregate type::

  struct Vector {
    double data[5];
  };

If we are differentiating a variable ``v`` of type ``Vector`` with respect to a variable ``x`` of type ``double``.
Then, the derivative is of the ``Vector`` type. Mathematically, let ``Vector`` represents
a vector space :math:`V`, then the following relations holds true:

.. math::
  x \in \mathbb{R} \\
  v \in V \\
  \pdv{v}{x} \in V

If :math:`\pdv{v}{x}` is stored in a variable `d_v`. Then we can access the individual 
derivatives as follows::

  d_v.data[0];  // derivative of v.data[0] w.r.t x
  d_v.data[1];  // derivative of v.data[1] w.r.t x
  .. and so on ..

Similarly, in the case of differentiating a variable ``y`` of type ``double`` with respect to a variable ``v`` of type ``Vector``,
the derivative, ``d_v``, is again of the ``Vector`` type. But the derivatives represented by the elements of ``d_v.data``` have changed.
In this case, the elements of ``d_v.data`` represent derivative of ``y`` with respect to each of the elements of ``v.data``.

If :math:`\pdv{x}{v}` is stored in a variable ``d_v``. Then we can access the individual derivatives as follows::

  d_v.data[0];  // derivative of x w.r.t v.data[0]
  d_v.data[1];  // derivative of x w.r.t v.data[1]
  .. and so on ..

Currently, class type support have the following limitations:

- Calls to member functions are not supported
- Calls to overloaded operators are not supported
- Initialising class type variables using non-default constructors that take 
  non-literal arguments (non-zero derivatives) are not supported.
- Class cannot have pointer or reference data members.

Class type support is under active development and thus, most of these 
limitations will be removed soon.

Specifying Custom Derivatives
-------------------------------

At times Clad may be unable to differentiate your function (e.g. if its definition is 
in a library and the source code is not available) or an efficient/more numerically 
stable expression for derivatives may be known that couldn't be computed just by applying 
the rules of automatic differentiation. In such cases, it is useful 
to be able to specify custom derivatives for the function.

Clad supports this functionality by allowing the user to specify their own custom derivatives
pushforward and pullback functions in the namespace ``clad::custom_derivatives::``. 
For a function ``FNAME`` one can specify:

- a custom derivative pushforward function by defining a function
  ``FNAME_pushforward`` inside the namespace ``clad::custom_derivatives::``.
- a custom derivative pullback function by defining a function
  ``FNAME_pullback`` inside the namespace ``clad::custom_derivatives::``.

When Clad will encounter a function ``FNAME``, it will first search for a 
suitable custom derivative function definition within the custom_derivatives namespace. 
Provided no definition was found, Clad will proceed to automatically derive the function.

Please read :ref:`Pushforward and Pullback Functions` section to get better understanding 
of them.

.. note::

   Currently, there is no way of specifying custom derivative function for 
   member functions. This limitation will be removed soon.

Example:

- Suppose that you have a function ``my_pow(x, y)`` which computes ``x`` to 
  the power of ``y``. In this example, Clad is not able to differentiate ``my_pow``'s 
  body 
  (e.g. it calls an external library or uses some non-differentiable approximation)::

    double my_pow(double x, double exponent) { // something non-differentiable here... }

However, the analytical formulas of its derivatives are known, thus one can easily 
specify custom derivatives::

  namespace clad {
    namespace custom_derivatives {
      double my_pow_pullback(double x, double exponent, double d_x,
                             double d_exponent) {
        return exponent * my_pow(x, exponent - 1) * d_x +
               (my_pow(x, exponent) * ::std::log(x)) * d_exponent;
      }
    } 
  } 

Moreover, a custom gradient can be specified::

  namespace clad {
    namespace custom_derivatives {
      void my_pow_pullback(double x, double exponent, double d_y,
                           clad::array_ref<double> d_x,
                           clad::array_ref<double> exponent) {
        double t = my_pow(x, exponent-1);
        *d_x += t * d_y;
        *d_exponent += t * x * ::std::log(x) * d_y;
      }
    }
  }

Whenever Clad will encounter ``my_pow`` inside a differentiated function, it 
will first try to find and use the provided custom derivative funtion before
attempting to automatically differentiate it.

.. note::
   Clad provides custom derivatives for some mathematical functions from ``<cmath>`` by default.


Numerical Differentiation Fallback
====================================


Error Estimation
======================

Debug functionalities
======================


