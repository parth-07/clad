Using Clad
***********

This section briefly describes all the key functionalities offered by Clad.
If you are just getting started with Clad, then this is the best place to start.
You may want to skim some sections on the first read. 

In case you haven't installed Clad already, then please do before proceeding 
with this guide. Visit :doc:`Clad installation and usage <InstallationAndUsage>` 
to know more about installing clad.

Let's get started.

Automatic Differentiation
===========================

Clad differentiation functions takes a function as an input and returns a 
`clad::CladFunction` object that contains information about the generated 
derived function. Generated derived function can be called by calling the 
`.execute` method on the corresponding `clad::CladFunction` object.

Clad provides 4 main automatic differentiation functions.

- `clad::differentiate` -- Primary forward mode automatic differentiation
- `clad::gradient` -- Primary reverse mode automatic differentiation
- `clad::hessian`
- `clad::jacobian`

Each of these functions will be explored in this guide.

.. todo::

   Perhaps add example use before proceeding with different differentiation modes.


Forward Mode Automatic Differentiation
----------------------------------------

Forward mode AD , also known as forward accumulation AD, allows to compute 
derivative of all the output variables with respect to a single input variable 
(independent variable) in a single pass of the generated derived function.
Mathematically, the forward mode AD allows us to efficiently compute columns of 
the jacobian matrix.

`clad::differentiate` provides the forward mode differentiation functionality. 
`clad::differentiate` function takes as input a 
source function and an independent parameter that can be specified either 
as the parameter index or the parameter name. It returns a `clad::CladFunction`
object containing the information about the generated derived function. 
Generated derived function can be executed by calling the `.execute` member 
function.

A self-explanatory example that demonstrates the usage of 
``clad::differentiate``::

   #include <iostream>
   #include "clad/Differentiator/Differentiator.h"

   double fn(double x, double y) {
     return x*x + y*y;
   }

   int main() {
     // differentiate 'fn' w.r.t 'x'.
     auto d_fn_1 = clad::differentiate(fn, "x");
  
     // computes derivative of 'fn' w.r.t 'x' when (x, y) = (3, 4).
     std::cout<<d_fn_1.execute(3, 4)<<"\n"; // prints 6
   }

Independent parameter can be specified either using the parameter name or
the parameter index. Indexing of the parameters starts from 0. Therefore for 
the example shown above, 2 differentiation calls shown below are
equivalent::

  clad::differentiate(fn, "x");

and:: 
  
  clad::differentiate(fn, 0);

We can execute the derived function by calling the `.execute` method. 
Clad forward mode differentiated functions returns the computed derivatives,
this behaviour is unique among other types of derived functions generated by Clad.

We can also differentiate with respect to an array element. The following
self-explanatory example demonstrates this::

  double fn_arr(double* arr, int n) {
    double res = 0;
    for (int i=0; i<n-1; ++i)
      res += arr[i] * arr[i+1];
    return res;
  }

  int main() {
    // Differentiate 'fn_arr' w.r.t element '1' of the 'arr' parameter
    auto d_fn_arr = clad::differentiate(fn_arr, "arr[1]");
    double arr[5] = {1, 2, 3, 4, 5};
    std::cout<<d_fn_arr.execute(arr, 5)<<"\n";  // prints 4
  }

Arbitrary higher-order derivatives can also be computed using the forward mode 
differentiation. The following examples demonstrates computing higher-order derivatives::

  double fn(double i) { return i * i * i * i; }

  int main() {
    // Differentiate 3rd order higher derivative of 'fn_arr' w.r.t parameter 'i'
    auto d_fn_3 = clad::differentiate<3>(fn, "i");
    std::cout << d_fn_3.execute(3) << "\n"; // prints 72.00
  }

.. note::

   Forward mode AD can only be used to differentiate with respect to a single 
   value. For differentiating with respect to multiple values (parameters), 
   please use reverse mode AD.
  
Visit API reference of :ref:`clad::differentiate<api_reference_clad_differentiate>`
for more details.

Reverse Mode Automatic Differentiation
----------------------------------------

Hessian Computation
----------------------

We can directly compute the 
`hessian matrix <https://en.wikipedia.org/wiki/Hessian_matrix>`_ of a
function in Clad using the ``clad::hessian`` function.

.. figure:: ../_static/hessian-matrix.png
  :width: 400
  :align: center
  :alt: Hessian matrix image taken from wikipedia
  
  Hessian matrix when specified parameters are 
  (x\ :sub:`1`\ , x\ :sub:`2`\ , ..., x\ :sub:`n`\ ).

`clad::hessian` provides the hessian computation functionality. 
`clad::hessian` function takes as input a source function, whose hessian have
to be computed, and optionally, information about independent variables.

Internally, ``clad::hessian`` uses both the forward mode AD and the 
reverse mode AD to efficiently compute hessian matrix.  

A self-explanatory example that demonstrates the usage of ``clad::hessian``::

  #include "clad/Differentiator/Differentiator.h"

  double kinetic_energy(double mass, double velocity) {
    return mass * velocity * velocity * 0.5;
  }

  int main() {
    // Generates all the second partial derivative columns of a Hessian matrix
    // and stores CallExprs to them inside a single function 
    auto hessian_one = clad::hessian(kinetic_energy);

    // Can manually specify independent arguments
    auto hessian_two = clad::hessian(kinetic_energy, "mass, velocity");

    // Creates an empty matrix to store the Hessian in
    // Must have enough space, 2 independent variables requires 4 elements (2^2=4)
    double matrix[4];

    // Prints the generated Hessian function
    hessian_one.dump();
    hessian_two.dump();

    // Substitutes these values into the Hessian function and pipes the result
    // into the matrix variable.
    hessian_one.execute(10, 2, matrix);
    hessian_two.execute(5, 1, matrix);
  }

Few important things to note about `clad::hessian`:

- If no independent variable information is provided, then hessian matrix is 
  computed by taking all differentiable function parameters as independent
  variables.

- Independent argument information is provided as a string literal with comma
  separated names of function parameters. For example::

    double product(double i, double j, double k) {
      return i*j*k;
    }
    // computes hessian matrix by taking parameter 'i' and 'j' as independent 
    // variables.
    auto d_fn = clad::hessian(product, "i, j");

- ``clad::hessian`` also supports differentiating w.r.t multiple paramters.

- Array that will store the computed hessian matrix should be passed as the 
  last argument to the call to the ``CladFunction::execute``. Array size 
  should atleast be as much as the size required to store the hessian matrix. 
  Passing array size less than the required size will result in undefined behaviour.

.. todo::

   Add details for computing hessian of array ranges.

Jacobian Computation
----------------------

Array Support 
----------------

Differentiating Functors and Lambdas
-------------------------------------

Despite significant differences, differentiating functors and lambda
expressions is remarkably similar to differentiating ordinary functions.

Similarly, computing hessian matrix and jacobian matrix of functors and
lambda expressions is also similar to computing hessian matrix and 
jacobian matrix of ordinary functions.

Differentiating functors and lambdas means differentiating the call operator 
(operator()) member function defined by the functor and lambda type and 
executing the differentiated function using a reference to the functor object.

A self-explanatory example that demonstrates the differentiation of functors::

  #include "clad/Differentiator/Differentiator.h"
  
  // A class type with user-defined call operator
  class Equation {
    double m_x, m_y;
  
    public:
    Equation(double x, double y) : m_x(x), m_y(y) {}
    double operator()(double i, double j) {
      return m_x*i*j + m_y*i*j;
    }
    void setX(double x) {
      m_x = x;
    }
  };
  
  int main() {
    Equation E(3, 5);
  
    // Functor is an object of any type which have user defined call operator.
    //
    // Clad differentiation functions can directly differentiate functors.
    // Functors can be passed to clad differentiation functions in two distinct
    // ways:
  
    // 1) Pass by reference
    // differentiates `E` wrt parameter `i`
    // object `E` is saved in the `CladFunction` object `d_E`
    auto d_E = clad::differentiate(E, "i");
  
    // 2) Pass as pointers
    // differentiates `E` wrt parameter `i`
    // object `E` is saved in the `CladFunction` object `d_E_pointer`
    auto d_E_pointer = clad::differentiate(&E, "i");
  
    // calculate differentiation of `E` when (i, j) = (7, 9)
    double res1 = d_E.execute(7, 9);  // prints 66
    double res2 = d_E_pointer.execute(7, 9);  // prints 66
  }

Functors and lambda expressions can be passed both by reference and by pointers.
Therefore, the two differentiation calls shown below are equivalent::

  Experiment E;  // a functor
  // passing function by reference
  auto d_E = clad::differentiate(E, "i");

and::

  Experiment E;  // a functor
  // passing function by pointer
  auto d_E = clad::differentiate(&E, "i");

An example that demonstrates differentiation of lambda expressions::

  int main() {
    auto lambda = [](double i, double j) {
      return i*j;
    };
    // Pass by reference
    auto lambda_grad = clad::gradient(lambda);
    // Can be passed by pointer as well!!
    auto lambda_grad_pointer = clad::gradient(&lambda);

    double d_i_1, d_j_1, d_i_2, d_j_2;
    d_i_1 = d_j_1 = d_i_2 = d_j_2 = 0;

    lambda_grad.execute(3, 5, &d_i_1, &d_j_1);
    lambda_grad_pointer.execute(3, 5, &d_i_2, &d_j_2);

    std::cout<<d_i_1<<" "<<d_j_1<<"\n"; // prints 5 3
    std::cout<<d_i_2<<" "<<d_j_2<<"\n"; // prints 5 3
  }

.. note::

   Functor class should not contain multiple overloaded call operators. 
   This restriction will be removed in the future.  

Differentiable Class Types
----------------------------

Custom Derivatives
---------------------

Numerical Differentiation Fallback
====================================


Error Estimation
======================

Debug functionalities
======================


